{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.config as config\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hopsworks\n",
    "\n",
    "# connect to the project\n",
    "project = hopsworks.login(\n",
    "    project=config.HOPSWORKS_PROJECT_NAME,\n",
    "    api_key_value=config.HOPSWORKS_API_KEY\n",
    ")\n",
    "\n",
    "# connect to the feature store\n",
    "feature_store = project.get_feature_store()\n",
    "\n",
    "# connect to the feature group\n",
    "feature_group = feature_store.get_feature_group(\n",
    "    name=config.FEATURE_GROUP_NAME,\n",
    "    version=config.FEATURE_GROUP_VERSION,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create feature view (if it doesn't exist yet)\n",
    "# This feature view only uses on feature group, so the query is trivial\n",
    "try:\n",
    "    # create feature view if it doesn't exist yet\n",
    "    feature_store.create_feature_view(\n",
    "        name=config.FEATURE_VIEW_NAME,\n",
    "        version=config.FEATURE_VIEW_VERSION,\n",
    "        query=feature_group.select_all()\n",
    "    )\n",
    "except:\n",
    "    print('Feature view already existed. Skip creation.')\n",
    "\n",
    "\n",
    "# get feature view\n",
    "feature_view = feature_store.get_feature_view(\n",
    "    name=config.FEATURE_VIEW_NAME,\n",
    "    version=config.FEATURE_VIEW_VERSION\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_data, _ = feature_view.training_data(\n",
    "    description='Time-series hourly taxi rides',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop `pickup_ts` column\n",
    "ts_data.drop('pickup_ts', axis=1, inplace=True)\n",
    "\n",
    "# sort by `pickup_location_id` and `pickup_hour`\n",
    "ts_data.sort_values(by=['pickup_location_id', 'pickup_hour'], inplace=True)\n",
    "ts_data[\"pickup_hour\"] = pd.to_datetime(ts_data[\"pickup_hour\"])\n",
    "ts_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from src.plot import plot_ts\n",
    "# from typing import Optional, List\n",
    "# import pandas as pd\n",
    "# import plotly.express as px \n",
    "\n",
    "# def plot_ts(\n",
    "#     ts_data: pd.DataFrame,\n",
    "#     locations: Optional[List[int]] = None\n",
    "#     ):\n",
    "#     \"\"\"\n",
    "#     Plot time-series data\n",
    "#     \"\"\"\n",
    "#     ts_data_to_plot = ts_data[ts_data.pickup_location_id.isin(locations)] if locations else ts_data\n",
    "\n",
    "#     fig = px.line(\n",
    "#         ts_data_to_plot,\n",
    "#         x=\"pickup_hour\",\n",
    "#         y=\"rides\",\n",
    "#         color='pickup_location_id',\n",
    "#         template='none',\n",
    "#     )\n",
    "\n",
    "#     fig.show()\n",
    "\n",
    "# plot_ts(ts_data, locations=[43])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import transform_ts_data_into_features_and_target\n",
    "\n",
    "features, targets = transform_ts_data_into_features_and_target(\n",
    "    ts_data,\n",
    "    input_seq_len=24*28, # one month\n",
    "    step_size=23,\n",
    ")\n",
    "\n",
    "features_and_target = features.copy()\n",
    "features_and_target['target_rides_next_hour'] = targets\n",
    "\n",
    "print(f'{features_and_target.shape=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date, timedelta\n",
    "from pytz import timezone\n",
    "from src.data_split import train_test_split\n",
    "\n",
    "# training data -> from January 2022 up until 2 months ago\n",
    "# test data -> last 2 months\n",
    "cutoff_date = pd.to_datetime(date.today() - timedelta(days=28*1), utc=True)\n",
    "\n",
    "print(f'{cutoff_date=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_and_target[\"pickup_hour\"] = pd.to_datetime(features_and_target[\"pickup_hour\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = train_test_split(\n",
    "    features_and_target,\n",
    "    cutoff_date,\n",
    "    target_column_name='target_rides_next_hour'   \n",
    ")\n",
    "\n",
    "print(f'{X_train.shape=}')\n",
    "print(f'{y_train.shape=}')\n",
    "print(f'{X_test.shape=}')\n",
    "print(f'{y_test.shape=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, TimeSeriesSplit\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import optuna\n",
    "\n",
    "from src.model import get_pipeline\n",
    "\n",
    "def objective(trial: optuna.trial.Trial) -> float:\n",
    "    \"\"\"\n",
    "    Given a set of hyper-parameters, it trains a model and computes an average\n",
    "    validation error based on a TimeSeriesSplit\n",
    "    \"\"\"\n",
    "    # pick hyper-parameters\n",
    "    hyperparams = {\n",
    "        \"metric\": 'mae',\n",
    "        \"verbose\": -1,\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n",
    "        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.2, 1.0),\n",
    "        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.2, 1.0),\n",
    "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 3, 100),   \n",
    "    }\n",
    "    \n",
    "    # sort X_train by `pikup_hour` inplace\n",
    "    # so the TimeSeriesSplit will split the data in a consistent way\n",
    "    X_train.sort_values('pickup_hour', inplace=True)\n",
    "\n",
    "    tss = TimeSeriesSplit(n_splits=2)\n",
    "    scores = []\n",
    "    for train_index, val_index in tss.split(X_train):\n",
    "\n",
    "        # split data for training and validation\n",
    "        X_train_, X_val_ = X_train.iloc[train_index, :], X_train.iloc[val_index,:]\n",
    "        y_train_, y_val_ = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "        \n",
    "        # train the model\n",
    "        pipeline = get_pipeline(**hyperparams)\n",
    "        pipeline.fit(X_train_, y_train_)\n",
    "        \n",
    "        # evaluate the model\n",
    "        y_pred = pipeline.predict(X_val_)\n",
    "        mae = mean_absolute_error(y_val_, y_pred)\n",
    "\n",
    "        scores.append(mae)\n",
    "   \n",
    "    # Return the mean score\n",
    "    return np.array(scores).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = study.best_trial.params\n",
    "print(f'{best_params=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = get_pipeline(**best_params)\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pipeline.predict(X_test)\n",
    "test_mae = mean_absolute_error(y_test, predictions)\n",
    "print(f'{test_mae=:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from src.paths import MODELS_DIR\n",
    "\n",
    "joblib.dump(pipeline, MODELS_DIR / 'model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hsml.schema import Schema\n",
    "from hsml.model_schema import ModelSchema\n",
    "\n",
    "input_schema = Schema(X_train)\n",
    "output_schema = Schema(y_train)\n",
    "model_schema = ModelSchema(input_schema=input_schema, output_schema=output_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_registry = project.get_model_registry()\n",
    "\n",
    "model = model_registry.sklearn.create_model(\n",
    "    name=\"taxi_demand_predictor_next_hour\",\n",
    "    metrics={\"test_mae\": test_mae},\n",
    "    description=\"LightGBM regressor with a bit of hyper-parameter tuning\",\n",
    "    input_example=X_train.sample(),\n",
    "    model_schema=model_schema\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6a271fb676f4c68a445a62208b3a37d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2485bde209747f69eb79635ab9dd9f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading: 0.000%|          | 0/305854 elapsed<00:00 remaining<?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0680155cc78748539fc686523b9da30e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading: 0.000%|          | 0/4381 elapsed<00:00 remaining<?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45c1a48d99864c368899a7fc7849d944",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading: 0.000%|          | 0/60849 elapsed<00:00 remaining<?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created, explore it at https://c.app.hopsworks.ai:443/p/1007769/models/taxi_demand_predictor_next_hour/1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Model(name: 'taxi_demand_predictor_next_hour', version: 1)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save(str(MODELS_DIR / 'model.pkl'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b98d97558a062384a76b0309256306c9ce5dd4e2074fe66c33532239207fc923"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
